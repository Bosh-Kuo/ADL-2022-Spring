{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset, load_metric\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_MAPPING,\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    EvalPrediction,\n",
    "    SchedulerType,\n",
    "    default_data_collator,\n",
    "    get_scheduler,\n",
    "    set_seed,\n",
    ")\n",
    "from utils_qa import postprocess_qa_predictions\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(config_name=None, dataset_config_name=None, dataset_name=None, do_eval=True, do_predict=False, do_train=True, doc_stride=128, gradient_accumulation_steps=8, learning_rate=3e-05, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, max_answer_length=30, max_eval_samples=100, max_predict_samples=None, max_seq_length=384, max_train_samples=100, max_train_steps=None, model_name_or_path='bert-base-chinese', n_best_size=20, null_score_diff_threshold=0.0, num_train_epochs=3, num_warmup_steps=0, output_csv='./data/test_submission.csv', output_dir='./qa', overwrite_cache=False, pad_to_max_length=True, per_device_eval_batch_size=4, per_device_train_batch_size=4, preprocessing_num_workers=4, seed=42, test_file='./data/QA_test.json', tokenizer_name=None, train_file='./data/QA_train.json', use_slow_tokenizer=False, validation_file='./data/QA_valid.json', version_2_with_negative=False, weight_decay=0.0)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(\n",
    "    description=\"Finetune a transformers model on a Question Answering task\")\n",
    "\n",
    "# data\n",
    "parser.add_argument(\n",
    "    \"--dataset_name\",\n",
    "    type=str,\n",
    "    default=None,\n",
    "    help=\"The name of the dataset to use (via the datasets library).\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--dataset_config_name\",\n",
    "    type=str,\n",
    "    default=None,\n",
    "    help=\"The configuration name of the dataset to use (via the datasets library).\",\n",
    ")\n",
    "\n",
    "parser.add_argument(\"--do_train\", action=\"store_true\",\n",
    "                    help=\"To do train on the question answering model\")\n",
    "parser.add_argument(\"--do_eval\", action=\"store_true\",\n",
    "                    help=\"To do eval on the question answering model\")\n",
    "parser.add_argument(\"--do_predict\", action=\"store_true\",\n",
    "                    help=\"To do prediction on the question answering model\")\n",
    "parser.add_argument(\n",
    "    \"--train_file\", type=str, default=\"./data/QA_train.json\", help=\"A csv or a json file containing the training data.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--validation_file\", type=str, default=\"./data/QA_valid.json\", help=\"A csv or a json file containing the validation data.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--test_file\", type=str, default=\"./data/QA_test.json\", help=\"A csv or a json file containing the Prediction data.\"\n",
    ")\n",
    "parser.add_argument(\"--output_dir\", type=str, default=\"./qa\",\n",
    "                    help=\"Where to store the final model.\")\n",
    "parser.add_argument(\n",
    "    \"--output_csv\", type=str, default=\"./data/test_submission.csv\", help=\"A csv or a json file containing the Prediction data.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_seq_length\",\n",
    "    type=int,\n",
    "    default=384,\n",
    "    help=\"The maximum total input sequence length after tokenization. Sequences longer than this will be truncated,\"\n",
    "    \" sequences shorter will be padded if `--pad_to_max_lengh` is passed.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--pad_to_max_length\",\n",
    "    action=\"store_true\",\n",
    "    help=\"If passed, pad all samples to `max_seq_length`. Otherwise, dynamic padding is used.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--preprocessing_num_workers\", type=int, default=4, help=\"preprocessing_num_workers\"\n",
    ")\n",
    "\n",
    "# model\n",
    "parser.add_argument(\n",
    "    \"--model_name_or_path\",\n",
    "    type=str,\n",
    "    default=\"bert-base-chinese\",\n",
    "    help=\"Path to pretrained model or model identifier from huggingface.co/models.\",\n",
    "    # required=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--config_name\",\n",
    "    type=str,\n",
    "    default=None,\n",
    "    help=\"Pretrained config name or path if not the same as model_name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--tokenizer_name\",\n",
    "    type=str,\n",
    "    default=None,\n",
    "    help=\"Pretrained tokenizer name or path if not the same as model_name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--use_slow_tokenizer\",\n",
    "    action=\"store_true\",\n",
    "    help=\"If passed, will use a slow tokenizer (not backed by the 🤗 Tokenizers library).\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--per_device_train_batch_size\",\n",
    "    type=int,\n",
    "    default=4,\n",
    "    help=\"Batch size (per device) for the training dataloader.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--per_device_eval_batch_size\",\n",
    "    type=int,\n",
    "    default=4,\n",
    "    help=\"Batch size (per device) for the evaluation dataloader.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--learning_rate\",\n",
    "    type=float,\n",
    "    default=3e-5,\n",
    "    help=\"Initial learning rate (after the potential warmup period) to use.\",\n",
    ")\n",
    "parser.add_argument(\"--weight_decay\", type=float,\n",
    "                    default=0.0, help=\"Weight decay to use.\")\n",
    "parser.add_argument(\"--num_train_epochs\", type=int, default=3,\n",
    "                    help=\"Total number of training epochs to perform.\")\n",
    "parser.add_argument(\n",
    "    \"--max_train_steps\",\n",
    "    type=int,\n",
    "    default=None,\n",
    "    help=\"Total number of training steps to perform. If provided, overrides num_train_epochs.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--gradient_accumulation_steps\",\n",
    "    type=int,\n",
    "    default=8,\n",
    "    help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--lr_scheduler_type\",\n",
    "    type=SchedulerType,\n",
    "    default=\"linear\",\n",
    "    help=\"The scheduler type to use.\",\n",
    "    choices=[\"linear\", \"cosine\", \"cosine_with_restarts\",\n",
    "             \"polynomial\", \"constant\", \"constant_with_warmup\"],\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_warmup_steps\", type=int, default=0, help=\"Number of steps for the warmup in the lr scheduler.\"\n",
    ")\n",
    "parser.add_argument(\"--seed\", type=int, default=42,\n",
    "                    help=\"A seed for reproducible training.\")\n",
    "parser.add_argument(\n",
    "    \"--doc_stride\",\n",
    "    type=int,\n",
    "    default=128,\n",
    "    help=\"When splitting up a long document into chunks how much stride to take between chunks.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--n_best_size\",\n",
    "    type=int,\n",
    "    default=20,\n",
    "    help=\"The total number of n-best predictions to generate when looking for an answer.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--null_score_diff_threshold\",\n",
    "    type=float,\n",
    "    default=0.0,\n",
    "    help=\"The threshold used to select the null answer: if the best answer has a score that is less than \"\n",
    "    \"the score of the null answer minus this threshold, the null answer is selected for this example. \"\n",
    "    \"Only useful when `version_2_with_negative=True`.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--version_2_with_negative\",\n",
    "    type=bool,\n",
    "    default=False,\n",
    "    help=\"If true, some of the examples do not have an answer.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_answer_length\",\n",
    "    type=int,\n",
    "    default=30,\n",
    "    help=\"The maximum length of an answer that can be generated. This is needed because the start \"\n",
    "    \"and end predictions are not conditioned on one another.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_train_samples\",\n",
    "    type=int,\n",
    "    default=None,\n",
    "    help=\"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
    "    \"value if set.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_eval_samples\",\n",
    "    type=int,\n",
    "    default=None,\n",
    "    help=\"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n",
    "    \"value if set.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_predict_samples\",\n",
    "    type=int,\n",
    "    default=None,\n",
    "    help=\"For debugging purposes or quicker training, truncate the number of prediction examples to this\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--overwrite_cache\", type=bool, default=False, help=\"Overwrite the cached training and evaluation sets\"\n",
    ")\n",
    "\n",
    "# train\n",
    "args = parser.parse_args(\n",
    "    args=[\n",
    "        \"--pad_to_max_length\",\n",
    "        \"--max_train_samples\", '100',\n",
    "        \"--max_eval_samples\", '100',\n",
    "        \"--do_train\",\n",
    "        \"--do_eval\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# test\n",
    "# args = parser.parse_args(\n",
    "#     args=[\n",
    "#     \"--pad_to_max_length\",\n",
    "#     \"--model_name_or_path\", './qa',\n",
    "#     \"--max_predict_samples\", '100',\n",
    "#     \"--do_predict\"]\n",
    "# )\n",
    "\n",
    "# Sanity checks\n",
    "if (\n",
    "    args.dataset_name is None\n",
    "    and args.train_file is None\n",
    "    and args.validation_file is None\n",
    "    and args.test_file is None\n",
    "):\n",
    "    raise ValueError(\n",
    "        \"Need either a dataset name or a training/validation/test file.\")\n",
    "else:\n",
    "    if args.train_file is not None:\n",
    "        extension = args.train_file.split(\".\")[-1]\n",
    "        assert extension in [\n",
    "            \"csv\", \"json\"], \"`train_file` should be a csv or a json file.\"\n",
    "    if args.validation_file is not None:\n",
    "        extension = args.validation_file.split(\".\")[-1]\n",
    "        assert extension in [\n",
    "            \"csv\", \"json\"], \"`validation_file` should be a csv or a json file.\"\n",
    "    if args.test_file is not None:\n",
    "        extension = args.test_file.split(\".\")[-1]\n",
    "        assert extension in [\n",
    "            \"csv\", \"json\"], \"`test_file` should be a csv or a json file.\"\n",
    "\n",
    "\n",
    "print(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize accelerator, logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/01/2022 01:26:56 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "Mixed precision type: no\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the accelerator. We will let the accelerator handle device placement for us in this example.\n",
    "accelerator = Accelerator()\n",
    "# Make one log on every process with the configuration for debugging.\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger.info(accelerator.state)\n",
    "\n",
    "# Setup logging, we only want one process per machine to log things on the screen.\n",
    "# accelerator.is_local_main_process is only True for one process per machine.\n",
    "logger.setLevel(logging.INFO if accelerator.is_local_main_process else logging.ERROR)\n",
    "if accelerator.is_local_main_process:\n",
    "    datasets.utils.logging.set_verbosity_warning()\n",
    "    transformers.utils.logging.set_verbosity_info()\n",
    "else:\n",
    "    datasets.utils.logging.set_verbosity_error()\n",
    "    transformers.utils.logging.set_verbosity_error()\n",
    "\n",
    "# If passed along, set the training seed now.\n",
    "if args.seed is not None:\n",
    "    set_seed(args.seed)\n",
    "\n",
    "# Handle the repository creation\n",
    "if accelerator.is_main_process:\n",
    "    if args.output_dir is not None:\n",
    "        os.makedirs(args.output_dir, exist_ok=True)\n",
    "accelerator.wait_for_everyone()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/01/2022 01:26:57 - WARNING - datasets.builder - Using custom data configuration default-8c4fdf57250df383\n",
      "04/01/2022 01:26:57 - WARNING - datasets.builder - Reusing dataset json (/home/bosh/.cache/huggingface/datasets/json/default-8c4fdf57250df383/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b)\n",
      "100%|██████████| 2/2 [00:00<00:00, 174.77it/s]\n"
     ]
    }
   ],
   "source": [
    "# Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)\n",
    "# or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n",
    "# (the dataset will be downloaded automatically from the datasets Hub).\n",
    "#\n",
    "# For CSV/JSON files, this script will use the column called 'text' or the first column if no column called\n",
    "# 'text' is found. You can easily tweak this behavior (see below).\n",
    "#\n",
    "# In distributed training, the load_dataset function guarantee that only one local process can concurrently\n",
    "# download the dataset.\n",
    "if args.dataset_name is not None:\n",
    "    # Downloading and loading a dataset from the hub.\n",
    "    raw_datasets = load_dataset(args.dataset_name, args.dataset_config_name)\n",
    "else:\n",
    "    data_files = {}\n",
    "    if args.train_file is not None and args.do_train:\n",
    "        data_files[\"train\"] = args.train_file\n",
    "        extension = args.train_file.split(\".\")[-1]\n",
    "    if args.validation_file is not None and args.do_eval:\n",
    "        data_files[\"validation\"] = args.validation_file\n",
    "        extension = args.validation_file.split(\".\")[-1]\n",
    "    if args.test_file is not None and args.do_predict:\n",
    "        data_files[\"test\"] = args.test_file\n",
    "        extension = args.test_file.split(\".\")[-1]\n",
    "    \n",
    "    raw_datasets = load_dataset(extension, data_files=data_files, field=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'question', 'paragraphs', 'relevant', 'answers', 'context'],\n",
       "        num_rows: 21714\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'question', 'paragraphs', 'relevant', 'answers', 'context'],\n",
       "        num_rows: 3009\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-chinese/resolve/main/config.json from cache at /home/bosh/.cache/huggingface/transformers/6cc404ca8136bc87bae0fb24f2259904943d776a6c5ddc26598bbdc319476f42.0f9bcd8314d841c06633e7b92b04509f1802c16796ee67b0f1177065739e24ae\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-chinese\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-chinese/resolve/main/config.json from cache at /home/bosh/.cache/huggingface/transformers/6cc404ca8136bc87bae0fb24f2259904943d776a6c5ddc26598bbdc319476f42.0f9bcd8314d841c06633e7b92b04509f1802c16796ee67b0f1177065739e24ae\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-chinese\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-chinese/resolve/main/vocab.txt from cache at /home/bosh/.cache/huggingface/transformers/36acdf4f3edf0a14ffb2b2c68ba47e93abd9448825202377ddb16dae8114fe07.accd894ff58c6ff7bd4f3072890776c14f4ea34fcc08e79cd88c2d157756dceb\n",
      "loading file https://huggingface.co/bert-base-chinese/resolve/main/tokenizer.json from cache at /home/bosh/.cache/huggingface/transformers/7e23f4e1f58f867d672f84d9a459826e41cea3be6d0fe62502ddce9920f57e48.4495f7812b44ff0568ce7c4ff3fdbb2bac5eaf330440ffa30f46893bf749184d\n",
      "loading file https://huggingface.co/bert-base-chinese/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-chinese/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-chinese/resolve/main/tokenizer_config.json from cache at /home/bosh/.cache/huggingface/transformers/2dc6085404c55008ba7fc09ab7483ef3f0a4ca2496ccee0cdbf51c2b5d529dff.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
      "loading configuration file https://huggingface.co/bert-base-chinese/resolve/main/config.json from cache at /home/bosh/.cache/huggingface/transformers/6cc404ca8136bc87bae0fb24f2259904943d776a6c5ddc26598bbdc319476f42.0f9bcd8314d841c06633e7b92b04509f1802c16796ee67b0f1177065739e24ae\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-chinese\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-chinese/resolve/main/pytorch_model.bin from cache at /home/bosh/.cache/huggingface/transformers/58592490276d9ed1e8e33f3c12caf23000c22973cb2b3218c641bd74547a1889.fabda197bfe5d6a318c2833172d6757ccc7e49f692cb949a6fabf560cee81508\n",
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained model and tokenizer\n",
    "#\n",
    "# In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n",
    "# download model & vocab.\n",
    "\n",
    "config = AutoConfig.from_pretrained(args.model_name_or_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, use_fast=True)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\n",
    "            args.model_name_or_path,\n",
    "            from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
    "            config=config,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the datasets.\n",
    "# Preprocessing is slighlty different for training and evaluation.\n",
    "\n",
    "question_column_name = \"question\" \n",
    "context_column_name = \"context\" \n",
    "answer_column_name = \"answers\" \n",
    "\n",
    "# Padding side determines if we do (question|context) or (context|question).\n",
    "pad_on_right = tokenizer.padding_side == \"right\"\n",
    "\n",
    "if args.max_seq_length > tokenizer.model_max_length:\n",
    "    logger.warning(\n",
    "        f\"The max_seq_length passed ({args.max_seq_length}) is larger than the maximum length for the\"\n",
    "        f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\"\n",
    "    )\n",
    "\n",
    "max_seq_length = min(args.max_seq_length, tokenizer.model_max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training preprocessing\n",
    "def prepare_train_features(examples):\n",
    "    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
    "    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
    "    # left whitespace\n",
    "    examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n",
    "\n",
    "    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[question_column_name if pad_on_right else context_column_name],\n",
    "        examples[context_column_name if pad_on_right else question_column_name],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_seq_length,\n",
    "        stride=args.doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\" if args.pad_to_max_length else False,\n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")  # ex: [0,1,2,2,3,4,4,4...]\n",
    "    # The offset mappings will give us a map from token to character position in the original context. This will\n",
    "    # help us compute the start_positions and end_positions.\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")  # ex: [off_0, off_1, off_2, off_2_1, off_3, off_4, off_4_1, off_4_2...]\n",
    "\n",
    "    # Let's label those examples!\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    #  若有些 QA context 太常會被分成好幾段，offset_mapping長度就是所有分段 QA input 的數量 \n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # We will label impossible answers with the index of the CLS token.\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        # None: 為特殊字符, 0: 該 token 屬於 question, 1: 該 token 屬於 context\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        # 第幾個 QA pair （一個 QA pair可以分成很多段，所以可能多段對應到同一個 QA pair）\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[answer_column_name][sample_index]\n",
    "        # If no answers are given, set the cls_index as answer.\n",
    "        # 此 QA 沒有給 answer就把 token start_positions, token end_positions 都定在cls的位置\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # Start/end character index of the answer in the text.\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            # Start token index of the current span in the text.\n",
    "            # 找出該 span 第一個 context token 的位置\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                token_start_index += 1\n",
    "\n",
    "            # End token index of the current span in the text.\n",
    "            # 找出該 span 最後一個 context token 的位置\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "            # 若答案不在一個過長的 QA 的其中一段，就把這段 token start_positions, token end_positions 都定在cls的位置\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # 確認 answer 一定在這段 span 的 context 中\n",
    "                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
    "                # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)  # 找出 answer start token 在該 span 的位置\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)  # 找出 answer end token 在該 span 的位置\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/01/2022 01:27:07 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/bosh/.cache/huggingface/datasets/json/default-8c4fdf57250df383/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b/cache-72a8a72add1db864.arrow\n",
      "04/01/2022 01:27:07 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/bosh/.cache/huggingface/datasets/json/default-8c4fdf57250df383/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b/cache-7f4c66b2f298ad9e.arrow\n",
      "04/01/2022 01:27:07 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/bosh/.cache/huggingface/datasets/json/default-8c4fdf57250df383/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b/cache-4e0c1715939c4de7.arrow\n",
      "04/01/2022 01:27:07 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/bosh/.cache/huggingface/datasets/json/default-8c4fdf57250df383/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b/cache-65f2e79c55d357c7.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "Dataset({\n",
      "    features: ['id', 'question', 'paragraphs', 'relevant', 'answers', 'context'],\n",
      "    num_rows: 100\n",
      "})\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
      "    num_rows: 173\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "if args.do_train:\n",
    "    if \"train\" not in raw_datasets:\n",
    "        raise ValueError(\"--do_train requires a train dataset\")\n",
    "    raw_train_dataset = raw_datasets[\"train\"]\n",
    "\n",
    "    # select train data\n",
    "    if args.max_train_samples is not None:\n",
    "        # We will select sample from whole data if agument is specified\n",
    "        raw_train_dataset = raw_train_dataset.select(range(args.max_train_samples))\n",
    "    print(\"\\n\\n\" + \"=\"*100 + \"\\n\\n\")\n",
    "    print(raw_train_dataset)\n",
    "\n",
    "    # Create train feature from dataset\n",
    "    with accelerator.main_process_first():\n",
    "        train_dataset = raw_train_dataset.map(\n",
    "            prepare_train_features,\n",
    "            batched=True,\n",
    "            num_proc=args.preprocessing_num_workers,\n",
    "            remove_columns=raw_datasets[\"train\"].column_names,\n",
    "            load_from_cache_file=not args.overwrite_cache,\n",
    "            desc=\"Running tokenizer on train dataset\",\n",
    "        )\n",
    "    print(\"\\n\\n\" + \"=\"*100 + \"\\n\\n\")\n",
    "    print(train_dataset)  # 一個 QA pair 可能會被切好幾段，所以數量可能會大於 raw_train_dataset\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_validation_features(examples):\n",
    "    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
    "    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
    "    # left whitespace\n",
    "    examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n",
    "\n",
    "    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[question_column_name if pad_on_right else context_column_name],\n",
    "        examples[context_column_name if pad_on_right else question_column_name],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_seq_length,\n",
    "        stride=args.doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\" if args.pad_to_max_length else False,\n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")  # ex: [0,1,2,2,3,4,4,4...]\n",
    "\n",
    "    # For evaluation, we will need to convert our predictions to substrings of the context, so we keep the\n",
    "    # corresponding example_id and we will store the offset mappings.\n",
    "    tokenized_examples[\"example_id\"] = []\n",
    "\n",
    "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        context_index = 1 if pad_on_right else 0\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "\n",
    "        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n",
    "        # position is part of the context or not.\n",
    "        # 原本 question tokens 和 特殊字符 tokens 也有 offset pair (startIdx, endIdx), 將除了 context 之外的 offset mapping 都轉成 None 就能快速分辨哪些是 context\n",
    "        tokenized_examples[\"offset_mapping\"][i] = [\n",
    "            (o if sequence_ids[k] == context_index else None)\n",
    "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "        ]\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "Dataset({\n",
      "    features: ['id', 'question', 'paragraphs', 'relevant', 'answers', 'context'],\n",
      "    num_rows: 100\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/01/2022 01:27:08 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/bosh/.cache/huggingface/datasets/json/default-8c4fdf57250df383/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b/cache-ed2b6e6797df23ac.arrow\n",
      "04/01/2022 01:27:08 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/bosh/.cache/huggingface/datasets/json/default-8c4fdf57250df383/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b/cache-5849039a9a303d88.arrow\n",
      "04/01/2022 01:27:08 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/bosh/.cache/huggingface/datasets/json/default-8c4fdf57250df383/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b/cache-d498db1a48db73ff.arrow\n",
      "04/01/2022 01:27:08 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/bosh/.cache/huggingface/datasets/json/default-8c4fdf57250df383/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b/cache-9220e0f601f0ad60.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'example_id'],\n",
      "    num_rows: 184\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "if args.do_eval:\n",
    "    if \"validation\" not in raw_datasets:\n",
    "        raise ValueError(\"--do_eval requires a validation dataset\")\n",
    "    raw_eval_dataset = raw_datasets[\"validation\"]\n",
    "    if args.max_eval_samples is not None:\n",
    "        # We will select sample from whole data\n",
    "        raw_eval_dataset = raw_eval_dataset.select(range(args.max_eval_samples))\n",
    "    print(\"\\n\\n\" + \"=\"*100 + \"\\n\\n\")\n",
    "    print(raw_eval_dataset)\n",
    "    # Validation Feature Creation\n",
    "    with accelerator.main_process_first():\n",
    "        eval_dataset = raw_eval_dataset.map(\n",
    "            prepare_validation_features,\n",
    "            batched=True,\n",
    "            num_proc=args.preprocessing_num_workers,\n",
    "            remove_columns=raw_datasets[\"validation\"].column_names,\n",
    "            load_from_cache_file=not args.overwrite_cache,\n",
    "            desc=\"Running tokenizer on validation dataset\",\n",
    "        )\n",
    "    print(\"\\n\\n\" + \"=\"*100 + \"\\n\\n\")\n",
    "    #  沒有 'start_positions', 'end_positions' 但有 'offset_mapping', 'example_id'\n",
    "    print(eval_dataset)  # 一個 QA pair 可能會被切好幾段，所以數量可能會大於 raw_eval_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create predict dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.do_predict:\n",
    "    if \"test\" not in raw_datasets:\n",
    "        raise ValueError(\"--do_predict requires a test dataset\")\n",
    "    raw_predict_dataset = raw_datasets[\"test\"]\n",
    "    if args.max_predict_samples is not None:\n",
    "        # We will select sample from whole data\n",
    "        raw_predict_dataset = raw_predict_dataset.select(range(args.max_predict_samples))\n",
    "    print(\"\\n\\n\" + \"=\"*100 + \"\\n\\n\")\n",
    "    print(raw_predict_dataset)\n",
    "    # Predict Feature Creation\n",
    "    with accelerator.main_process_first():\n",
    "        predict_dataset = raw_predict_dataset.map(\n",
    "            prepare_validation_features,\n",
    "            batched=True,\n",
    "            num_proc=args.preprocessing_num_workers,\n",
    "            remove_columns=raw_datasets[\"test\"].column_names,\n",
    "            load_from_cache_file=not args.overwrite_cache,\n",
    "            desc=\"Running tokenizer on prediction dataset\",\n",
    "        )\n",
    "    print(\"\\n\\n\" + \"=\"*100 + \"\\n\\n\")\n",
    "    #  沒有 'start_positions', 'end_positions' 但有 'offset_mapping', 'example_id'\n",
    "    print(predict_dataset)  # 一個 QA pair 可能會被切好幾段，所以數量可能會大於 raw_eval_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoaders creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.pad_to_max_length:\n",
    "    # If padding was already done ot max length, we use the default data collator that will just convert everything\n",
    "    # to tensors.\n",
    "    data_collator = default_data_collator\n",
    "else:\n",
    "    # Otherwise, `DataCollatorWithPadding` will apply dynamic padding for us (by padding to the maximum length of\n",
    "    # the samples passed). When using mixed precision, we add `pad_to_multiple_of=8` to pad all tensors to multiple\n",
    "    # of 8s, which will enable the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
    "    data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=(8 if accelerator.use_fp16 else None))\n",
    "\n",
    "if args.do_train and args.do_eval:\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size\n",
    "    )\n",
    "\n",
    "    eval_dataset_for_model = eval_dataset.remove_columns([\"example_id\", \"offset_mapping\"])\n",
    "    eval_dataloader = DataLoader(\n",
    "        eval_dataset_for_model, shuffle=False, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size\n",
    "    )\n",
    "    metric = load_metric(\"squad_v2\" if args.version_2_with_negative else \"squad\")\n",
    "\n",
    "if args.do_predict:\n",
    "    predict_dataset_for_model = predict_dataset.remove_columns([\"example_id\", \"offset_mapping\"])\n",
    "    predict_dataloader = DataLoader(\n",
    "        predict_dataset_for_model, shuffle=False, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_processing_function(examples, features, predictions, stage=\"eval\"):\n",
    "    # Post-processing: we match the start logits and end logits to answers in the original context.\n",
    "    predictions = postprocess_qa_predictions(\n",
    "        examples=examples,\n",
    "        features=features,\n",
    "        predictions=predictions,\n",
    "        version_2_with_negative=args.version_2_with_negative,\n",
    "        n_best_size=args.n_best_size,\n",
    "        max_answer_length=args.max_answer_length,\n",
    "        null_score_diff_threshold=args.null_score_diff_threshold,\n",
    "        output_dir=args.output_dir,\n",
    "        prefix=stage,\n",
    "    )\n",
    "    # Format the result to the format the metric expects.\n",
    "    if args.version_2_with_negative:\n",
    "        formatted_predictions = [\n",
    "            {\"id\": k, \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in predictions.items()\n",
    "        ]\n",
    "    else:\n",
    "        formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in predictions.items()]\n",
    "\n",
    "    if stage == \"eval\":\n",
    "        references = [{\"id\": ex[\"id\"], \"answers\": ex[answer_column_name]} for ex in examples]\n",
    "        return EvalPrediction(predictions=formatted_predictions, label_ids=references)\n",
    "    elif stage == \"predict\":\n",
    "        return formatted_predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fill numpy array of size len_of_validation_data * max_length_of_output_tensor\n",
    "def create_and_fill_np_array(start_or_end_logits, dataset, max_len):\n",
    "    \"\"\"\n",
    "    Create and fill numpy array of size len_of_validation_data * max_length_of_output_tensor\n",
    "    Args:\n",
    "        start_or_end_logits(:obj:`tensor`):\n",
    "            This is the output predictions of the model. We can only enter either start or end logits.\n",
    "        eval_dataset: Evaluation dataset\n",
    "        max_len(:obj:`int`):\n",
    "            The maximum length of the output tensor. ( See the model.eval() part for more details )\n",
    "    \"\"\"\n",
    "\n",
    "    step = 0\n",
    "    # create a numpy array and fill it with -100.\n",
    "    logits_concat = np.full((len(dataset), max_len), -100, dtype=np.float64)\n",
    "    # Now since we have create an array now we will populate it with the outputs gathered using accelerator.gather\n",
    "    for i, output_logit in enumerate(start_or_end_logits):  # populate columns\n",
    "        # We have to fill it such that we have to take the whole tensor and replace it on the newly created array\n",
    "        # And after every iteration we have to change the step\n",
    "\n",
    "        batch_size = output_logit.shape[0]\n",
    "        cols = output_logit.shape[1]\n",
    "\n",
    "        if step + batch_size < len(dataset):\n",
    "            logits_concat[step : step + batch_size, :cols] = output_logit\n",
    "        else:\n",
    "            logits_concat[step:, :cols] = output_logit[: len(dataset) - step]\n",
    "\n",
    "        step += batch_size\n",
    "\n",
    "    return logits_concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bosh/anaconda3/envs/adl/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Optimizer\n",
    "# Split weights in two groups, one with weight decay and the other not.\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": args.weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n",
    "\n",
    "if args.do_train and args.do_eval:\n",
    "    # Prepare everything with our `accelerator`.\n",
    "    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, eval_dataloader\n",
    "    )\n",
    "\n",
    "if args.do_predict:\n",
    "    model, optimizer, predict_dataloader = accelerator.prepare(\n",
    "    model, optimizer, predict_dataloader\n",
    ")\n",
    "\n",
    "# Note -> the training dataloader needs to be prepared before we grab his length below (cause its length will be\n",
    "# shorter in multiprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.do_train:    \n",
    "    # Scheduler and math around the number of training steps.\n",
    "    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
    "    if args.max_train_steps is None:\n",
    "        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
    "    else:\n",
    "        args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n",
    "\n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=args.lr_scheduler_type,\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=args.num_warmup_steps,\n",
    "        num_training_steps=args.max_train_steps,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/01/2022 01:27:12 - INFO - __main__ - ***** Running training *****\n",
      "04/01/2022 01:27:12 - INFO - __main__ -   Num examples = 173\n",
      "04/01/2022 01:27:12 - INFO - __main__ -   Num Epochs = 3\n",
      "04/01/2022 01:27:12 - INFO - __main__ -   Instantaneous batch size per device = 4\n",
      "04/01/2022 01:27:12 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "04/01/2022 01:27:12 - INFO - __main__ -   Gradient Accumulation steps = 8\n",
      "04/01/2022 01:27:12 - INFO - __main__ -   Total optimization steps = 18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 7/18 [00:03<00:03,  2.80it/s]04/01/2022 01:27:15 - INFO - __main__ - ***** Running Evaluation *****\n",
      "04/01/2022 01:27:15 - INFO - __main__ -   Num examples = 184\n",
      "04/01/2022 01:27:15 - INFO - __main__ -   Batch size = 4\n",
      "100%|██████████| 100/100 [00:00<00:00, 283.74it/s]\n",
      "04/01/2022 01:27:16 - INFO - __main__ - Evaluation metrics: {'exact_match': 7.0, 'f1': 7.0}\n",
      "Configuration saved in ./qa/config.json\n",
      "Model weights saved in ./qa/pytorch_model.bin\n",
      " 78%|███████▊  | 14/18 [00:08<00:01,  2.31it/s]04/01/2022 01:27:20 - INFO - __main__ - ***** Running Evaluation *****\n",
      "04/01/2022 01:27:20 - INFO - __main__ -   Num examples = 184\n",
      "04/01/2022 01:27:20 - INFO - __main__ -   Batch size = 4\n",
      "100%|██████████| 100/100 [00:00<00:00, 281.47it/s]\n",
      "04/01/2022 01:27:22 - INFO - __main__ - Evaluation metrics: {'exact_match': 10.0, 'f1': 10.0}\n",
      "Configuration saved in ./qa/config.json\n",
      "Model weights saved in ./qa/pytorch_model.bin\n",
      "100%|██████████| 18/18 [00:12<00:00,  1.46it/s]04/01/2022 01:27:24 - INFO - __main__ - ***** Running Evaluation *****\n",
      "04/01/2022 01:27:24 - INFO - __main__ -   Num examples = 184\n",
      "04/01/2022 01:27:24 - INFO - __main__ -   Batch size = 4\n",
      "100%|██████████| 100/100 [00:00<00:00, 277.57it/s]\n",
      "04/01/2022 01:27:26 - INFO - __main__ - Evaluation metrics: {'exact_match': 11.0, 'f1': 11.0}\n",
      "Configuration saved in ./qa/config.json\n",
      "Model weights saved in ./qa/pytorch_model.bin\n",
      "tokenizer config file saved in ./qa/tokenizer_config.json\n",
      "Special tokens file saved in ./qa/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "if args.do_train and args.do_eval:\n",
    "    # Train!\n",
    "    total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n",
    "\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
    "    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n",
    "    logger.info(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n",
    "    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
    "    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
    "    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n",
    "    print(\"\\n\\n\" + \"=\"*100 + \"\\n\\n\")\n",
    "\n",
    "    # Only show the progress bar once on each machine.\n",
    "    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n",
    "    completed_steps = 0\n",
    "\n",
    "    eval_EM = []\n",
    "    for epoch in range(args.num_train_epochs):\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss = loss / args.gradient_accumulation_steps\n",
    "            accelerator.backward(loss)\n",
    "            if step % args.gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                progress_bar.update(1)\n",
    "                completed_steps += 1\n",
    "\n",
    "            if completed_steps >= args.max_train_steps:\n",
    "                break\n",
    "\n",
    "\n",
    "        # Evaluation\n",
    "        print(\"\\n\\n\" + \"=\" * 100 + \"\\n\\n\")\n",
    "        logger.info(\"***** Running Evaluation *****\")\n",
    "        logger.info(f\"  Num examples = {len(eval_dataset)}\")\n",
    "        logger.info(f\"  Batch size = {args.per_device_eval_batch_size}\")\n",
    "\n",
    "        all_start_logits = []\n",
    "        all_end_logits = []\n",
    "        for step, batch in enumerate(eval_dataloader):\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "                start_logits = outputs.start_logits\n",
    "                end_logits = outputs.end_logits\n",
    "\n",
    "                if not args.pad_to_max_length:  # necessary to pad predictions and labels for being gathered\n",
    "                    start_logits = accelerator.pad_across_processes(start_logits, dim=1, pad_index=-100)\n",
    "                    end_logits = accelerator.pad_across_processes(end_logits, dim=1, pad_index=-100)\n",
    "\n",
    "                all_start_logits.append(accelerator.gather(start_logits).cpu().numpy())\n",
    "                all_end_logits.append(accelerator.gather(end_logits).cpu().numpy())\n",
    "\n",
    "        max_len = max([x.shape[1] for x in all_start_logits])  # Get the max_length of the tensor\n",
    "\n",
    "        # concatenate the numpy array\n",
    "        start_logits_concat = create_and_fill_np_array(all_start_logits, eval_dataset, max_len)\n",
    "        end_logits_concat = create_and_fill_np_array(all_end_logits, eval_dataset, max_len)\n",
    "\n",
    "        # delete the list of numpy arrays\n",
    "        del all_start_logits\n",
    "        del all_end_logits\n",
    "\n",
    "        outputs_numpy = (start_logits_concat, end_logits_concat)\n",
    "        prediction = post_processing_function(raw_eval_dataset, eval_dataset, outputs_numpy)\n",
    "        eval_metric = metric.compute(predictions=prediction.predictions, references=prediction.label_ids)\n",
    "        logger.info(f\"Evaluation metrics: {eval_metric}\")\n",
    "        eval_EM.append(eval_metric[\"exact_match\"])\n",
    "\n",
    "        # save check point\n",
    "        if args.output_dir and epoch < args.num_train_epochs - 1:\n",
    "            accelerator.wait_for_everyone()\n",
    "            unwrapped_model = accelerator.unwrap_model(model)\n",
    "            unwrapped_model.save_pretrained(args.output_dir, save_function=accelerator.save)\n",
    "\n",
    "    # save model\n",
    "    if args.output_dir is not None:\n",
    "        np.save(f'{args.output_dir}/eval_EM', np.array(eval_EM))\n",
    "        accelerator.wait_for_everyone()\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        unwrapped_model.save_pretrained(args.output_dir, save_function=accelerator.save)\n",
    "        if accelerator.is_main_process:\n",
    "            tokenizer.save_pretrained(args.output_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "if args.do_predict:\n",
    "    logger.info(\"***** Running Prediction *****\")\n",
    "    logger.info(f\"  Num examples = {len(predict_dataset)}\")\n",
    "    logger.info(f\"  Batch size = {args.per_device_eval_batch_size}\")\n",
    "\n",
    "    all_start_logits = []\n",
    "    all_end_logits = []\n",
    "    for step, batch in enumerate(predict_dataloader):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "            start_logits = outputs.start_logits\n",
    "            end_logits = outputs.end_logits\n",
    "\n",
    "            if not args.pad_to_max_length:  # necessary to pad predictions and labels for being gathered\n",
    "                start_logits = accelerator.pad_across_processes(start_logits, dim=1, pad_index=-100)\n",
    "                end_logits = accelerator.pad_across_processes(start_logits, dim=1, pad_index=-100)\n",
    "\n",
    "            all_start_logits.append(accelerator.gather(start_logits).cpu().numpy())\n",
    "            all_end_logits.append(accelerator.gather(end_logits).cpu().numpy())\n",
    "\n",
    "    max_len = max([x.shape[1] for x in all_start_logits])  # Get the max_length of the tensor\n",
    "    # concatenate the numpy array\n",
    "    start_logits_concat = create_and_fill_np_array(all_start_logits, predict_dataset, max_len)\n",
    "    end_logits_concat = create_and_fill_np_array(all_end_logits, predict_dataset, max_len)\n",
    "\n",
    "    # delete the list of numpy arrays\n",
    "    del all_start_logits\n",
    "    del all_end_logits\n",
    "\n",
    "    outputs_numpy = (start_logits_concat, end_logits_concat)\n",
    "\n",
    "    # [{\"id\": k, \"prediction_text\": v} for k, v in predictions.items()]\n",
    "    predictions = post_processing_function(raw_predict_dataset, predict_dataset, outputs_numpy, stage=\"predict\")\n",
    "\n",
    "    # write csv file\n",
    "    with open(args.output_csv, 'w') as f:\n",
    "            f.write('id,answer\\n')\n",
    "            for i, prediction in enumerate(predictions):\n",
    "                f.write(f'{prediction[\"id\"]},{prediction[\"prediction_text\"]}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aaed0f13cab955ee754a66aa5a48de1ce31e05bf25215437ce503315f7004fd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('adl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
