{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from itertools import chain\n",
    "from pathlib import Path\n",
    "from typing import Optional, Union\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "from datasets import load_dataset, load_metric\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "from huggingface_hub import Repository\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_MAPPING,\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelForMultipleChoice,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedTokenizerBase,\n",
    "    SchedulerType,\n",
    "    default_data_collator,\n",
    "    get_scheduler,\n",
    "    set_seed,\n",
    ")\n",
    "# from transformers.utils import get_full_repo_name\n",
    "from transformers.file_utils import PaddingStrategy\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(config_name=None, context_file='./data/context.json', dataset_config_name=None, dataset_name=None, debug=False, do_eval=False, do_test=True, do_train=False, gradient_accumulation_steps=4, learning_rate=3e-05, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, max_length=384, max_train_steps=None, model_name_or_path='./multiple_choice', num_train_epochs=3, num_warmup_steps=0, output_dir='./multiple_choice', output_file='./data/QA_test.json', pad_to_max_length=True, per_device_eval_batch_size=4, per_device_train_batch_size=4, seed=42, test_file='./data/multiple_choice_valid.json', tokenizer_name=None, train_file='./data/multiple_choice_train.json', use_slow_tokenizer=False, validation_file='./data/multiple_choice_valid.json', weight_decay=0.0)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(\n",
    "    description=\"Finetune a transformers model on a text classification task\")\n",
    "# Data\n",
    "parser.add_argument(\n",
    "    \"--dataset_name\",\n",
    "    type=str,\n",
    "    default=None,\n",
    "    help=\"The name of the dataset to use (via the datasets library).\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--dataset_config_name\",\n",
    "    type=str,\n",
    "    default=None,\n",
    "    help=\"The configuration name of the dataset to use (via the datasets library).\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--train_file\", type=str, default='./data/multiple_choice_train.json', help=\"A csv or a json file containing the training data.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--validation_file\", type=str, default='./data/multiple_choice_valid.json', help=\"A csv or a json file containing the validation data.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--test_file\", type=str, default='./data/multiple_choice_valid.json', help=\"A csv or a json file containing the test data.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--output_file\", type=str, default='./data/QA_test.json', help=\"A csv or a json file containing the predicted test data.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--context_file\", type=str, default='./data/context.json', help=\"A csv or a json file containing the context data.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_length\",\n",
    "    type=int,\n",
    "    default=384,\n",
    "    help=(\n",
    "        \"The maximum total input sequence length after tokenization. Sequences longer than this will be truncated,\"\n",
    "        \" sequences shorter will be padded if `--pad_to_max_lengh` is passed.\"\n",
    "    ),\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--pad_to_max_length\",\n",
    "    action=\"store_true\",\n",
    "    help=\"If passed, pad all samples to `max_length`. Otherwise, dynamic padding is used.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--do_train\",\n",
    "    action=\"store_true\",\n",
    "    help=\"If passed, go through the trian process\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--do_eval\",\n",
    "    action=\"store_true\",\n",
    "    help=\"If passed, go through the validate process\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--do_test\",\n",
    "    action=\"store_true\",\n",
    "    help=\"If passed, go through the test process\",\n",
    ")\n",
    "\n",
    "# Model\n",
    "parser.add_argument(\n",
    "    \"--model_name_or_path\",\n",
    "    type=str,\n",
    "    default=\"bert-base-chinese\",\n",
    "    help=\"Path to pretrained model or model identifier from huggingface.co/models.\",\n",
    "    # required=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--config_name\",\n",
    "    type=str,\n",
    "    default=None,\n",
    "    help=\"Pretrained config name or path if not the same as model_name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--tokenizer_name\",\n",
    "    type=str,\n",
    "    default=None,\n",
    "    help=\"Pretrained tokenizer name or path if not the same as model_name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--use_slow_tokenizer\",\n",
    "    action=\"store_true\",\n",
    "    help=\"If passed, will use a slow tokenizer (not backed by the 🤗 Tokenizers library).\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--per_device_train_batch_size\",\n",
    "    type=int,\n",
    "    default=4,\n",
    "    help=\"Batch size (per device) for the training dataloader.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--per_device_eval_batch_size\",\n",
    "    type=int,\n",
    "    default=4,\n",
    "    help=\"Batch size (per device) for the evaluation dataloader.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--learning_rate\",\n",
    "    type=float,\n",
    "    default=3e-5,\n",
    "    help=\"Initial learning rate (after the potential warmup period) to use.\",\n",
    ")\n",
    "parser.add_argument(\"--weight_decay\", type=float,\n",
    "                    default=0.0, help=\"Weight decay to use.\")\n",
    "parser.add_argument(\"--num_train_epochs\", type=int, default=3,\n",
    "                    help=\"Total number of training epochs to perform.\")\n",
    "parser.add_argument(\n",
    "    \"--max_train_steps\",\n",
    "    type=int,\n",
    "    default=None,\n",
    "    help=\"Total number of training steps to perform. If provided, overrides num_train_epochs.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--gradient_accumulation_steps\",\n",
    "    type=int,\n",
    "    default=4,\n",
    "    help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--lr_scheduler_type\",\n",
    "    type=SchedulerType,\n",
    "    default=\"linear\",\n",
    "    help=\"The scheduler type to use.\",\n",
    "    choices=[\"linear\", \"cosine\", \"cosine_with_restarts\",\n",
    "             \"polynomial\", \"constant\", \"constant_with_warmup\"],\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_warmup_steps\", type=int, default=0, help=\"Number of steps for the warmup in the lr scheduler.\"\n",
    ")\n",
    "parser.add_argument(\"--output_dir\", type=str, default=\"./multiple_choice\",\n",
    "                    help=\"Where to store the final model.\")\n",
    "parser.add_argument(\"--seed\", type=int, default=42,\n",
    "                    help=\"A seed for reproducible training.\")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--debug\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Activate debug mode and run training only with a subset of data.\",\n",
    ")\n",
    "\n",
    "# args = parser.parse_args(\n",
    "#     args=['--pad_to_max_length', '--do_train', '--do_eval', '--debug']\n",
    "# )\n",
    "args = parser.parse_args(\n",
    "    args=['--pad_to_max_length', '--model_name_or_path', './multiple_choice', '--do_test',]\n",
    ")\n",
    "print(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataCollatorForMultipleChoice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForMultipleChoice:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs for multiple choice received.\n",
    "    Args:\n",
    "        tokenizer ([`PreTrainedTokenizer`] or [`PreTrainedTokenizerFast`]):\n",
    "            The tokenizer used for encoding the data.\n",
    "        padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single sequence\n",
    "              if provided).\n",
    "            - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
    "              acceptable input length for the model if that argument is not provided.\n",
    "            - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
    "              lengths).\n",
    "        max_length (`int`, *optional*):\n",
    "            Maximum length of the returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (`int`, *optional*):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features):\n",
    "        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n",
    "        labels = [feature.pop(label_name) for feature in features]\n",
    "        batch_size = len(features)\n",
    "        num_choices = len(features[0][\"input_ids\"])\n",
    "        flattened_features = [\n",
    "            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n",
    "        ]\n",
    "        flattened_features = list(chain(*flattened_features))\n",
    "\n",
    "        batch = self.tokenizer.pad(\n",
    "            flattened_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # Un-flatten\n",
    "        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n",
    "        # Add back labels\n",
    "        batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/31/2022 22:42:29 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "Mixed precision type: no\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the accelerator. We will let the accelerator handle device placement for us in this example.\n",
    "accelerator = Accelerator()\n",
    "# Make one log on every process with the configuration for debugging.\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger.info(accelerator.state)\n",
    "\n",
    "# Setup logging, we only want one process per machine to log things on the screen.\n",
    "# accelerator.is_local_main_process is only True for one process per machine.\n",
    "logger.setLevel(\n",
    "    logging.INFO if accelerator.is_local_main_process else logging.ERROR)\n",
    "if accelerator.is_local_main_process:\n",
    "    datasets.utils.logging.set_verbosity_warning()\n",
    "    transformers.utils.logging.set_verbosity_info()\n",
    "else:\n",
    "    datasets.utils.logging.set_verbosity_error()\n",
    "    transformers.utils.logging.set_verbosity_error()\n",
    "\n",
    "# If passed along, set the training seed now.\n",
    "if args.seed is not None:\n",
    "    set_seed(args.seed)\n",
    "\n",
    "# Handle the repository creation\n",
    "if accelerator.is_main_process:\n",
    "    if args.output_dir is not None:\n",
    "        os.makedirs(args.output_dir, exist_ok=True)\n",
    "accelerator.wait_for_everyone()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/31/2022 22:42:30 - WARNING - datasets.builder - Using custom data configuration default-725f49bce73f8354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/bosh/.cache/huggingface/datasets/json/default-725f49bce73f8354/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 1482.09it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 566.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/bosh/.cache/huggingface/datasets/json/default-725f49bce73f8354/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 754.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "\n",
      "\n",
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['id', 'question', 'paragraphs', 'relevant', 'answer'],\n",
      "        num_rows: 3009\n",
      "    })\n",
      "})\n",
      "{'id': ['9f0860355c55e113d615cc88b3fa9420', '20f49d82038169b184336c4c1f96f64d', '44fbf143b338305c5c553fc733b1fe54'], 'question': ['伊利諾大學是哪一個國家的大學?', '中華人民共和國成立後，土地改革運動由誰所主持帶領?', '哈康七世海、拉扎耶夫海、戴維斯海、莫森海均有一部份位於哪一洋的範圍內？'], 'paragraphs': [[6037, 4555, 5641, 1029], [7814, 735, 5583, 2420], [4997, 5950, 6017, 389]], 'relevant': [4555, 735, 389], 'answer': [{'start': 15, 'text': '美國'}, {'start': 433, 'text': '劉少奇'}, {'start': 0, 'text': '南冰洋'}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)\n",
    "# or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n",
    "# (the dataset will be downloaded automatically from the datasets Hub).\n",
    "#\n",
    "# For CSV/JSON files, this script will use the column called 'text' or the first column if no column called\n",
    "# 'text' is found. You can easily tweak this behavior (see below).\n",
    "#\n",
    "# In distributed training, the load_dataset function guarantee that only one local process can concurrently\n",
    "# download the dataset.\n",
    "\n",
    "# 載入官方dataset\n",
    "if args.dataset_name is not None:\n",
    "    # Downloading and loading a dataset from the hub.\n",
    "    raw_datasets = load_dataset(args.dataset_name, args.dataset_config_name)\n",
    "else:\n",
    "    data_files = {}\n",
    "    if args.train_file is not None and args.do_train:\n",
    "        data_files[\"train\"] = args.train_file\n",
    "        extension = args.train_file.split(\".\")[-1]   # \"json\"\n",
    "    if args.validation_file is not None and args.do_eval:\n",
    "        data_files[\"validation\"] = args.validation_file\n",
    "        extension = args.validation_file.split(\".\")[-1]   # \"json\"\n",
    "    if args.test_file is not None and args.do_test:\n",
    "        data_files[\"test\"] = args.test_file\n",
    "        extension = args.test_file.split(\".\")[-1]   # \"json\"\n",
    "    raw_datasets = load_dataset(extension, data_files=data_files, field='data')\n",
    "\n",
    " # Trim a number of training examples 取 100 個 data 來用，正式訓練或預測時關掉\n",
    "if args.debug:\n",
    "    for split in raw_datasets.keys():\n",
    "        raw_datasets[split] = raw_datasets[split].select(range(100))\n",
    "\n",
    "if args.do_train:\n",
    "    print(\"=\"*100 + \"\\n\\n\")\n",
    "    print(raw_datasets)\n",
    "    print(raw_datasets['train'][0:3])\n",
    "if args.do_test:\n",
    "    print(\"=\"*100 + \"\\n\\n\")\n",
    "    print(raw_datasets)\n",
    "    print(raw_datasets['test'][0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load context list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9013\n"
     ]
    }
   ],
   "source": [
    "# 載入 context.json list\n",
    "context_dir = Path(args.context_file)\n",
    "context_list = json.loads(context_dir.read_text())\n",
    "print(len(context_list))  # 9013"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## download model & vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./multiple_choice/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"./multiple_choice\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMultipleChoice\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "Didn't find file ./multiple_choice/added_tokens.json. We won't load it.\n",
      "loading file ./multiple_choice/vocab.txt\n",
      "loading file ./multiple_choice/tokenizer.json\n",
      "loading file None\n",
      "loading file ./multiple_choice/special_tokens_map.json\n",
      "loading file ./multiple_choice/tokenizer_config.json\n",
      "loading weights file ./multiple_choice/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForMultipleChoice.\n",
      "\n",
      "All the weights of BertForMultipleChoice were initialized from the model checkpoint at ./multiple_choice.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMultipleChoice for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(21128, 768, padding_idx=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n",
    "# download model & vocab.\n",
    "config = AutoConfig.from_pretrained(args.model_name_or_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, use_fast=not args.use_slow_tokenizer)\n",
    "model = AutoModelForMultipleChoice.from_pretrained(\n",
    "            args.model_name_or_path,\n",
    "            from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
    "            config=config,\n",
    "        )\n",
    "model.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check max sequence length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 確保自訂的 max seq length 不超過 model 提供的 max seq len\n",
    "if args.max_length is None:\n",
    "    max_length = tokenizer.model_max_length\n",
    "    if max_length > 1024:\n",
    "        logger.warning(\n",
    "            f\"The tokenizer picked seems to have a very large `model_max_length` ({tokenizer.model_max_length}). \"\n",
    "            \"Picking 1024 instead. You can change that default value by passing --max_seq_length xxx.\"\n",
    "        )\n",
    "        max_length = 1024\n",
    "else:\n",
    "    if args.max_length > tokenizer.model_max_length:\n",
    "        logger.warning(\n",
    "            f\"The max_seq_length passed ({args.max_length}) is larger than the maximum length for the\"\n",
    "            f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\"\n",
    "        )\n",
    "    max_length = min(args.max_length, tokenizer.model_max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding = \"max_length\" if args.pad_to_max_length else False\n",
    "# Preprocessing the datasets.\n",
    "# When using your own dataset or a different dataset from swag, you will probably need to change this.\n",
    "# ending_names = [f\"ending{i}\" for i in range(4)]\n",
    "question_name = \"question\"\n",
    "paragraphs_name = \"paragraphs\"\n",
    "relevant_name = \"relevant\"\n",
    "def preprocess_function(examples):\n",
    "    # Repeat each first sentence four times to go with the four possibilities of second sentences.\n",
    "    # question: 前文\n",
    "    # [[question_0, question_0, question_0, question_0], [question_1, question_1, question_1, question_1], ... [question_N, question_N, question_N, question_N]]\n",
    "    first_sentences = [[question] * 4 for question in examples[question_name]]\n",
    "\n",
    "    # Grab all paragraph context possible for each question.\n",
    "    paragraphsIdx = examples[paragraphs_name]\n",
    "\n",
    "    # paragraph: 後文（選項）\n",
    "    # [[paragraphs0_0, paragraphs1_0, paragraphs2_0, paragraphs3_0], [paragraphs0_1, paragraphs1_1, paragraphs2_1, paragraphs3_1] ...]\n",
    "    second_sentences = [\n",
    "        [f\"{context_list[idx]}\" for idx in selections] for i, selections in enumerate(paragraphsIdx)  # i: 第 i 個 data\n",
    "    ]\n",
    "\n",
    "    # Flatten everything\n",
    "    # [question_0, question_0, question_0, question_0, question_1, question_1, question_1, question_1, ... question_N, question_N, question_N, question_N]\n",
    "    first_sentences = sum(first_sentences, [])\n",
    "    # [paragraphs0_0, paragraphs1_0, paragraphs2_0, paragraphs3_0, paragraphs0_1, paragraphs1_1, paragraphs2_1, paragraphs3_1 ...]\n",
    "    second_sentences = sum(second_sentences, [])\n",
    "\n",
    "    # Tokenize\n",
    "    tokenized_examples = tokenizer(\n",
    "        first_sentences,\n",
    "        second_sentences,\n",
    "        truncation=True,\n",
    "        max_length=args.max_length,\n",
    "        padding=\"max_length\" if args.pad_to_max_length else False,\n",
    "    )\n",
    "    # Tokenize\n",
    "    # {\n",
    "    #  [token_dataId_選項]: [CLS] question [SEP] paragraph(選項i) [SEP]  (轉為 idx 的 list)  \n",
    "    # 'input_ids': [[tokens_0_0], [tokens_0_1], [tokens_0_2], [tokens_0_3], ... [tokens_N_0], [tokens_N_1], [tokens_N_2], [tokens_N_3]]} => (N-1) * 4\n",
    "    # 'token_type_ids: [[tokens_0_0], [tokens_0_1], [tokens_0_2], [tokens_0_3], ... [tokens_N_0], [tokens_N_1], [tokens_N_2], [tokens_N_3]]} => (N-1) * 4\n",
    "    # 'attention_mask': [[tokens_0_0], [tokens_0_1], [tokens_0_2], [tokens_0_3], ... [tokens_N_0], [tokens_N_1], [tokens_N_2], [tokens_N_3]]} => (N-1) * 4\n",
    "    # }\n",
    "\n",
    "    # Un-flatten\n",
    "    # input_ids': [[[tokens_0_0], [tokens_0_1], [tokens_0_2], [tokens_0_3]], ... [[tokens_N_0], [tokens_N_1], [tokens_N_2], [tokens_N_3]]]} => (N-1)\n",
    "    encoded_examples = {k: [v[i : i + 4] for i in range(0, len(v), 4)] for k, v in tokenized_examples.items()}\n",
    "\n",
    "    # 將 encoded_examples 加入 label\n",
    "    if relevant_name in examples.keys():\n",
    "        labels = examples[relevant_name]\n",
    "        encoded_examples['label'] = [selections.index(labels[i]) for i, selections in enumerate(paragraphsIdx)]\n",
    "    else:\n",
    "        # for test data\n",
    "        encoded_examples['label'] = [0 for i, selections in enumerate(paragraphsIdx)]\n",
    "    return encoded_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess train dataset / valid dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'question', 'paragraphs', 'relevant', 'answer'],\n",
      "    num_rows: 3009\n",
      "})\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:03<00:00,  1.03ba/s]\n"
     ]
    }
   ],
   "source": [
    "# do train\n",
    "if args.do_train:\n",
    "    if \"train\" not in raw_datasets:\n",
    "        raise ValueError(\"--do_train requires a train dataset\")\n",
    "    raw_train_dataset = raw_datasets[\"train\"]\n",
    "    print(raw_train_dataset)\n",
    "\n",
    "    with accelerator.main_process_first():\n",
    "        print(\"\\n\\n\" + \"=\"*100 + \"\\n\\n\")\n",
    "        train_dataset = raw_train_dataset.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            remove_columns=raw_datasets[\"train\"].column_names\n",
    "        )\n",
    "# do validate\n",
    "if args.do_eval:\n",
    "    if \"validation\" not in raw_datasets:\n",
    "        raise ValueError(\"--do_eval requires a validation dataset\")\n",
    "    raw_eval_dataset = raw_datasets[\"validation\"]\n",
    "    print(raw_eval_dataset)\n",
    "    with accelerator.main_process_first():\n",
    "        print(\"\\n\\n\" + \"=\"*100 + \"\\n\\n\")\n",
    "        eval_dataset = raw_eval_dataset.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            remove_columns=raw_datasets[\"train\"].column_names\n",
    "        )\n",
    "\n",
    "# do test\n",
    "if args.do_test:\n",
    "    if \"test\" not in raw_datasets:\n",
    "        raise ValueError(\"--do_test requires a test dataset\")\n",
    "    raw_test_dataset = raw_datasets[\"test\"]\n",
    "    print(raw_test_dataset)\n",
    "    with accelerator.main_process_first():\n",
    "        print(\"\\n\\n\" + \"=\"*100 + \"\\n\\n\")\n",
    "        test_dataset = raw_test_dataset.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            remove_columns=raw_datasets[\"test\"].column_names\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'question', 'paragraphs', 'relevant', 'answer']\n",
      "['input_ids', 'token_type_ids', 'attention_mask', 'label']\n"
     ]
    }
   ],
   "source": [
    "if args.do_train:\n",
    "    print(list(raw_train_dataset[0].keys()))\n",
    "    print(list(train_dataset[0].keys()))\n",
    "if args.do_eval:\n",
    "    print(list(raw_eval_dataset[0].keys()))\n",
    "    print(list(eval_dataset[0].keys()))\n",
    "if args.do_test:\n",
    "    print(list(raw_test_dataset[0].keys()))\n",
    "    print(list(test_dataset[0].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaders creation:\n",
    "if args.pad_to_max_length:\n",
    "    # If padding was already done ot max length, we use the default data collator that will just convert everything\n",
    "    # to tensors.\n",
    "    data_collator = default_data_collator\n",
    "else:\n",
    "    # Otherwise, `DataCollatorWithPadding` will apply dynamic padding for us (by padding to the maximum length of\n",
    "    # the samples passed). When using mixed precision, we add `pad_to_multiple_of=8` to pad all tensors to multiple\n",
    "    # of 8s, which will enable the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
    "    data_collator = DataCollatorForMultipleChoice(\n",
    "        tokenizer, pad_to_multiple_of=(8 if accelerator.use_fp16 else None)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': tensor([1, 1, 3, 2]), 'input_ids': tensor([[[ 101,  823, 1164,  ..., 8024,  699,  102],\n",
      "         [ 101,  823, 1164,  ...,    0,    0,    0],\n",
      "         [ 101,  823, 1164,  ...,    0,    0,    0],\n",
      "         [ 101,  823, 1164,  ...,  897,  961,  102]],\n",
      "\n",
      "        [[ 101,  704, 5836,  ..., 5681, 2342,  102],\n",
      "         [ 101,  704, 5836,  ..., 2251,  511,  102],\n",
      "         [ 101,  704, 5836,  ...,  510, 1385,  102],\n",
      "         [ 101,  704, 5836,  ...,    0,    0,    0]],\n",
      "\n",
      "        [[ 101, 1506, 2434,  ...,    0,    0,    0],\n",
      "         [ 101, 1506, 2434,  ..., 4638, 6211,  102],\n",
      "         [ 101, 1506, 2434,  ...,    0,    0,    0],\n",
      "         [ 101, 1506, 2434,  ...,    0,    0,    0]],\n",
      "\n",
      "        [[ 101, 1921,  712,  ..., 1762,  519,  102],\n",
      "         [ 101, 1921,  712,  ...,    0,    0,    0],\n",
      "         [ 101, 1921,  712,  ...,    0,    0,    0],\n",
      "         [ 101, 1921,  712,  ..., 2341,  677,  102]]]), 'token_type_ids': tensor([[[0, 0, 0,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 1, 1, 1]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 1, 1, 1]]]), 'attention_mask': tensor([[[1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 1, 1, 1]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 1, 1, 1]]])}\n"
     ]
    }
   ],
   "source": [
    "if args.do_train:\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size\n",
    "    )\n",
    "    print(next(iter(train_dataloader)))\n",
    "if args.do_eval:\n",
    "    eval_dataloader = DataLoader(\n",
    "        eval_dataset, shuffle=False, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size\n",
    "    )\n",
    "    print(next(iter(eval_dataloader)))\n",
    "if args.do_test:\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset, shuffle=False, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size\n",
    "    )\n",
    "    print(next(iter(test_dataloader)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bosh/anaconda3/envs/adl/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Split weights in two groups, one with weight decay and the other not.\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": args.weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the device given by the `accelerator` object.\n",
    "device = accelerator.device\n",
    "model.to(device)\n",
    "\n",
    "# Prepare everything with our `accelerator`.\n",
    "if args.do_train and args.do_eval:\n",
    "    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, eval_dataloader\n",
    "    )\n",
    "if args.do_test:\n",
    "    model, optimizer, test_dataloader = accelerator.prepare(model, optimizer, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## learning rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note -> the training dataloader needs to be prepared before we grab his length below (cause its length will be\n",
    "    # shorter in multiprocess)\n",
    "\n",
    "# Scheduler and math around the number of training steps.\n",
    "if args.do_train:\n",
    "    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
    "    if args.max_train_steps is None:\n",
    "        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch  # 訓練過程總步數\n",
    "    else:\n",
    "        args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n",
    "\n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=args.lr_scheduler_type,\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=args.num_warmup_steps,\n",
    "        num_training_steps=args.max_train_steps,\n",
    "    )\n",
    "# Metrics\n",
    "metric = load_metric(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.do_train and args.do_eval:   \n",
    "    # Train!\n",
    "    total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n",
    "\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
    "    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n",
    "    logger.info(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n",
    "    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
    "    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
    "    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n",
    "    print(\"\\n\\n\" + \"=\"*100 + \"\\n\\n\")\n",
    "\n",
    "    # Only show the progress bar once on each machine.\n",
    "    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n",
    "    completed_steps = 0\n",
    "\n",
    "    for epoch in range(args.num_train_epochs):\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss = loss / args.gradient_accumulation_steps\n",
    "            accelerator.backward(loss)\n",
    "            if step % args.gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                progress_bar.update(1)\n",
    "                completed_steps += 1\n",
    "\n",
    "            if completed_steps >= args.max_train_steps:\n",
    "                break\n",
    "\n",
    "        model.eval()\n",
    "        eval_Accuracy = []\n",
    "        for step, batch in enumerate(eval_dataloader):\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "            predictions = outputs.logits.argmax(dim=-1)\n",
    "            metric.add_batch(\n",
    "                predictions=accelerator.gather(predictions),\n",
    "                references=accelerator.gather(batch[\"labels\"]),\n",
    "            )\n",
    "            eval_metric = metric.compute()\n",
    "            eval_Accuracy.append(eval_metric[\"accuracy\"])\n",
    "\n",
    "        # eval_metric = metric.compute()\n",
    "        accelerator.print(f\"epoch {epoch}: {eval_metric}\")\n",
    "\n",
    "        # if args.push_to_hub and epoch < args.num_train_epochs - 1:\n",
    "        if epoch < args.num_train_epochs - 1:\n",
    "            accelerator.wait_for_everyone()\n",
    "            unwrapped_model = accelerator.unwrap_model(model)\n",
    "            unwrapped_model.save_pretrained(args.output_dir, save_function=accelerator.save)\n",
    "            if accelerator.is_main_process:\n",
    "                tokenizer.save_pretrained(args.output_dir)\n",
    "\n",
    "\n",
    "    if args.output_dir is not None:\n",
    "        np.save(f'{args.output_dir}/eval_accuracy', np.array(eval_Accuracy))\n",
    "        accelerator.wait_for_everyone()\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        unwrapped_model.save_pretrained(args.output_dir, save_function=accelerator.save)\n",
    "        if accelerator.is_main_process:\n",
    "                tokenizer.save_pretrained(args.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 753/753 [00:56<00:00, 13.42it/s]\n",
      "100%|██████████| 3009/3009 [00:30<00:00, 100.02it/s]\n"
     ]
    }
   ],
   "source": [
    "if args.do_test:\n",
    "    output_json = {\"data\":[]}\n",
    "    predictions_idx = []\n",
    "    model.eval()\n",
    "    for step, batch in enumerate(tqdm(test_dataloader)):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        predictions = predictions.cpu().tolist()\n",
    "        for prediction_idx in predictions:\n",
    "            predictions_idx.append(prediction_idx)\n",
    "    \n",
    "    for i, pred in enumerate(tqdm(predictions_idx)):\n",
    "        context_id = raw_test_dataset[\"paragraphs\"][i][pred]\n",
    "        data = {\n",
    "            \"id\": raw_test_dataset[\"id\"][i],\n",
    "            \"question\": raw_test_dataset[\"question\"][i],\n",
    "            \"context\": context_list[context_id]\n",
    "        }\n",
    "        output_json[\"data\"].append(data)\n",
    "    json.dump(output_json, open(args.output_file, 'w'), indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aaed0f13cab955ee754a66aa5a48de1ce31e05bf25215437ce503315f7004fd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('adl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
