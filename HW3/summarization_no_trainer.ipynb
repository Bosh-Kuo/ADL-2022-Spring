{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bosh/anaconda3/envs/adl/lib/python3.8/site-packages/ckiptagger/model_ws.py:106: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  cell = tf.compat.v1.nn.rnn_cell.LSTMCell(hidden_d, name=name)\n",
      "/home/bosh/anaconda3/envs/adl/lib/python3.8/site-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:984: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  self._kernel = self.add_variable(\n",
      "/home/bosh/anaconda3/envs/adl/lib/python3.8/site-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:993: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  self._bias = self.add_variable(\n",
      "2022-04-30 18:21:31.010363: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-04-30 18:21:31.010391: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: bosh-Lab514\n",
      "2022-04-30 18:21:31.010396: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: bosh-Lab514\n",
      "2022-04-30 18:21:31.010458: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: NOT_FOUND: was unable to find libcuda.so DSO loaded into this program\n",
      "2022-04-30 18:21:31.010481: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 510.60.2\n",
      "2022-04-30 18:21:32.244993: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import datasets\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset, load_metric\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import set_seed\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    SchedulerType,\n",
    "    get_scheduler,\n",
    ")\n",
    "\n",
    "from tw_rouge import get_rouge\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(config_name=None, debug=False, do_eval=False, do_train=True, gradient_accumulation_steps=4, ignore_pad_token_for_loss=True, learning_rate=5e-05, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, max_length=128, max_source_length=256, max_target_length=64, max_train_steps=None, model_name_or_path='google/mt5-small', num_beams=5, num_train_epochs=1, num_warmup_steps=0, output_dir='./mt5_small_model', overwrite_cache=True, pad_to_max_length=True, per_device_eval_batch_size=4, per_device_train_batch_size=4, preprocessing_num_workers=None, seed=31, source_prefix=None, summary_column=None, text_column=None, tokenizer_name=None, train_file='./data/train.jsonl', use_slow_tokenizer=False, val_max_target_length=64, validation_file='./data/public.jsonl', weight_decay=0.0)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(\n",
    "    description=\"Finetune a transformers model on a summarization task\")\n",
    "parser.add_argument(\n",
    "    \"--do_train\",\n",
    "    action=\"store_true\",\n",
    "    help=\"If passed, go through the trian process\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--do_eval\",\n",
    "    action=\"store_true\",\n",
    "    help=\"If passed, go through the test process\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--train_file\", type=str, default=\"./data/train.jsonl\", help=\"A csv or a json file containing the training data.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--validation_file\", type=str, default=\"./data/public.jsonl\", help=\"A csv or a json file containing the validation data.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ignore_pad_token_for_loss\",\n",
    "    type=bool,\n",
    "    default=True,\n",
    "    help=\"Whether to ignore the tokens corresponding to \" \"padded labels in the loss computation or not.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_source_length\",\n",
    "    type=int,\n",
    "    default=256,\n",
    "    help=\"The maximum total input sequence length after \"\n",
    "    \"tokenization.Sequences longer than this will be truncated, sequences shorter will be padded.\",\n",
    ")\n",
    "# for t5 series model \"summarize: \"\n",
    "parser.add_argument(\n",
    "    \"--source_prefix\",\n",
    "    type=str,\n",
    "    default=None,\n",
    "    help=\"A prefix to add before every source text \" \"(useful for T5 models).\",\n",
    ")\n",
    "# ÂèØ‰∏çË®≠\n",
    "# Max number of processes when generating cache. Already cached shards are loaded sequentially\n",
    "parser.add_argument(\n",
    "    \"--preprocessing_num_workers\",\n",
    "    type=int,\n",
    "    default=None,\n",
    "    help=\"The number of processes to use for the preprocessing.\",\n",
    ")\n",
    "# not use the cached files and force the preprocessing to be applied every times\n",
    "parser.add_argument(\n",
    "    \"--overwrite_cache\", type=bool, default=True, help=\"Overwrite the cached training and evaluation sets\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_target_length\",\n",
    "    type=int,\n",
    "    default=64,\n",
    "    help=\"The maximum total sequence length for target text after \"\n",
    "    \"tokenization. Sequences longer than this will be truncated, sequences shorter will be padded.\"\n",
    "    \"during ``evaluate`` and ``predict``.\",\n",
    ")\n",
    "\n",
    "# tutorial Áî® beam search\n",
    "parser.add_argument(\n",
    "    \"--num_beams\",\n",
    "    type=int,\n",
    "    default=5,\n",
    "    help=\"Number of beams to use for evaluation. This argument will be \"\n",
    "    \"passed to ``model.generate``, which is used during ``evaluate`` and ``predict``.\",\n",
    ")\n",
    "# Ë¶ÅÂä†\n",
    "parser.add_argument(\n",
    "    \"--pad_to_max_length\",\n",
    "    action=\"store_true\",\n",
    "    help=\"If passed, pad all samples to `max_length`. Otherwise, dynamic padding is used.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--model_name_or_path\",\n",
    "    type=str,\n",
    "    default=\"google/mt5-small\",\n",
    "    help=\"Path to pretrained model or model identifier from huggingface.co/models.\",\n",
    "    # required=True,\n",
    ")\n",
    "# ‰∏çÂä†\n",
    "parser.add_argument(\n",
    "    \"--use_slow_tokenizer\",\n",
    "    action=\"store_true\",\n",
    "    help=\"If passed, will use a slow tokenizer (not backed by the ü§ó Tokenizers library).\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--per_device_train_batch_size\",\n",
    "    type=int,\n",
    "    default=4,\n",
    "    help=\"Batch size (per device) for the training dataloader.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--per_device_eval_batch_size\",\n",
    "    type=int,\n",
    "    default=4,\n",
    "    help=\"Batch size (per device) for the evaluation dataloader.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--learning_rate\",\n",
    "    type=float,\n",
    "    default=5e-5,\n",
    "    help=\"Initial learning rate (after the potential warmup period) to use.\",\n",
    ")\n",
    "parser.add_argument(\"--weight_decay\", type=float,\n",
    "                    default=0.0, help=\"Weight decay to use.\")\n",
    "parser.add_argument(\"--num_train_epochs\", type=int, default=1,\n",
    "                    help=\"Total number of training epochs to perform.\")\n",
    "parser.add_argument(\n",
    "    \"--max_train_steps\",\n",
    "    type=int,\n",
    "    default=None,\n",
    "    help=\"Total number of training steps to perform. If provided, overrides num_train_epochs.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--gradient_accumulation_steps\",\n",
    "    type=int,\n",
    "    default=4,\n",
    "    help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--lr_scheduler_type\",\n",
    "    type=SchedulerType,\n",
    "    default=\"linear\",\n",
    "    help=\"The scheduler type to use.\",\n",
    "    choices=[\"linear\", \"cosine\", \"cosine_with_restarts\",\n",
    "             \"polynomial\", \"constant\", \"constant_with_warmup\"],\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_warmup_steps\", type=int, default=0, help=\"Number of steps for the warmup in the lr scheduler.\"\n",
    ")\n",
    "parser.add_argument(\"--output_dir\", type=str, default=\"./mt5_small_model\",\n",
    "                    help=\"Where to store the final model.\")\n",
    "parser.add_argument(\"--seed\", type=int, default=31,\n",
    "                    help=\"A seed for reproducible training.\")\n",
    "parser.add_argument(\n",
    "    \"--debug\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Activate debug mode and run training only with a subset of data.\",\n",
    ")\n",
    "\n",
    "\n",
    "args = parser.parse_args(\n",
    "    # args=['--pad_to_max_length', '--do_train', '--debug']\n",
    "    args=['--pad_to_max_length', '--do_train',]\n",
    ")\n",
    "print(args)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Accelerator and logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/30/2022 18:21:32 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "Mixed precision type: no\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the accelerator. We will let the accelerator handle device placement for us in this example.\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# Make one log on every process with the configuration for debugging.\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger.info(accelerator.state)\n",
    "\n",
    "# Setup logging, we only want one process per machine to log things on the screen.\n",
    "# accelerator.is_local_main_process is only True for one process per machine.\n",
    "logger.setLevel(\n",
    "    logging.INFO if accelerator.is_local_main_process else logging.ERROR)\n",
    "if accelerator.is_local_main_process:\n",
    "    datasets.utils.logging.set_verbosity_warning()\n",
    "    transformers.utils.logging.set_verbosity_info()\n",
    "else:\n",
    "    datasets.utils.logging.set_verbosity_error()\n",
    "    transformers.utils.logging.set_verbosity_error()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set seed and check output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If passed along, set the training seed now.\n",
    "if args.seed is not None:\n",
    "    set_seed(args.seed)\n",
    "\n",
    "# Handle the repository creation\n",
    "if accelerator.is_main_process:\n",
    "    if args.output_dir is not None:\n",
    "        os.makedirs(args.output_dir, exist_ok=True)\n",
    "accelerator.wait_for_everyone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/30/2022 18:21:34 - WARNING - datasets.builder - Using custom data configuration default-d107d6620dd251dd\n",
      "04/30/2022 18:21:34 - WARNING - datasets.builder - Reusing dataset json (/home/bosh/.cache/huggingface/datasets/json/default-d107d6620dd251dd/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b)\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 151.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['date_publish', 'title', 'source_domain', 'maintext', 'split', 'id'],\n",
      "        num_rows: 21710\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['date_publish', 'title', 'source_domain', 'maintext', 'split', 'id'],\n",
      "        num_rows: 5494\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_files = {}\n",
    "if args.train_file is not None:\n",
    "    data_files[\"train\"] = args.train_file\n",
    "if args.validation_file is not None:\n",
    "    data_files[\"validation\"] = args.validation_file\n",
    "raw_datasets = load_dataset(\"json\", data_files=data_files)\n",
    "\n",
    " # Trim a number of training examples Âèñ 100 ÂÄã data ‰æÜÁî®ÔºåÊ≠£ÂºèË®ìÁ∑¥ÊàñÈ†êÊ∏¨ÊôÇÈóúÊéâ\n",
    "if args.debug:\n",
    "    for split in raw_datasets.keys():\n",
    "        raw_datasets[split] = raw_datasets[split].select(range(100))\n",
    "\n",
    "print(raw_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/google/mt5-small/resolve/main/config.json from cache at /home/bosh/.cache/huggingface/transformers/97693496c1a0cae463bd18428187f9e9924d2dfbadaa46e4d468634a0fc95a41.dadce13f8f85f4825168354a04675d4b177749f8f11b167e87676777695d4fe4\n",
      "Model config MT5Config {\n",
      "  \"_name_or_path\": \"google/mt5-small\",\n",
      "  \"architectures\": [\n",
      "    \"MT5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 1024,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"mt5\",\n",
      "  \"num_decoder_layers\": 8,\n",
      "  \"num_heads\": 6,\n",
      "  \"num_layers\": 8,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"T5Tokenizer\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250112\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/google/mt5-small/resolve/main/config.json from cache at /home/bosh/.cache/huggingface/transformers/97693496c1a0cae463bd18428187f9e9924d2dfbadaa46e4d468634a0fc95a41.dadce13f8f85f4825168354a04675d4b177749f8f11b167e87676777695d4fe4\n",
      "Model config MT5Config {\n",
      "  \"_name_or_path\": \"google/mt5-small\",\n",
      "  \"architectures\": [\n",
      "    \"MT5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 1024,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"mt5\",\n",
      "  \"num_decoder_layers\": 8,\n",
      "  \"num_heads\": 6,\n",
      "  \"num_layers\": 8,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"T5Tokenizer\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250112\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/google/mt5-small/resolve/main/spiece.model from cache at /home/bosh/.cache/huggingface/transformers/37d0f67f084f8c5fc5589e0bba5ff3c6307af833bb0b7f4eb33fbfd8d4038a9d.84ea7af2df68dc8db434d3160aab65cce8ac63ce5b6f7743f8c9a4a14b4f77e2\n",
      "loading file https://huggingface.co/google/mt5-small/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/google/mt5-small/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/google/mt5-small/resolve/main/special_tokens_map.json from cache at /home/bosh/.cache/huggingface/transformers/685ac0ca8568ec593a48b61b0a3c272beee9bc194a3c7241d15dcadb5f875e53.f76030f3ec1b96a8199b2593390c610e76ca8028ef3d24680000619ffb646276\n",
      "loading file https://huggingface.co/google/mt5-small/resolve/main/tokenizer_config.json from cache at /home/bosh/.cache/huggingface/transformers/6a9e52d6dd21568e37b65fc180ada927968e8f7124f0acd6efcaf90cd2e0f4bb.4b81e5d952ad810ca1de2b3e362b9a26a5cc77b4b75daf20caf69fb838751c32\n",
      "loading configuration file https://huggingface.co/google/mt5-small/resolve/main/config.json from cache at /home/bosh/.cache/huggingface/transformers/97693496c1a0cae463bd18428187f9e9924d2dfbadaa46e4d468634a0fc95a41.dadce13f8f85f4825168354a04675d4b177749f8f11b167e87676777695d4fe4\n",
      "Model config MT5Config {\n",
      "  \"_name_or_path\": \"google/mt5-small\",\n",
      "  \"architectures\": [\n",
      "    \"MT5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 1024,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"mt5\",\n",
      "  \"num_decoder_layers\": 8,\n",
      "  \"num_heads\": 6,\n",
      "  \"num_layers\": 8,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"T5Tokenizer\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250112\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/google/mt5-small/resolve/main/config.json from cache at /home/bosh/.cache/huggingface/transformers/97693496c1a0cae463bd18428187f9e9924d2dfbadaa46e4d468634a0fc95a41.dadce13f8f85f4825168354a04675d4b177749f8f11b167e87676777695d4fe4\n",
      "Model config MT5Config {\n",
      "  \"_name_or_path\": \"google/mt5-small\",\n",
      "  \"architectures\": [\n",
      "    \"MT5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 1024,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"mt5\",\n",
      "  \"num_decoder_layers\": 8,\n",
      "  \"num_heads\": 6,\n",
      "  \"num_layers\": 8,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"T5Tokenizer\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250112\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/google/mt5-small/resolve/main/pytorch_model.bin from cache at /home/bosh/.cache/huggingface/transformers/8e7b2a80ddcb5611b27d8c89e1e8e33a947e105415051402a22b9c8d7d1caeb0.e22331f3a065b885b30ae3dd1ff11ccaf7fbc444485f6eb07ef5e0138bca8b70\n",
      "All model checkpoint weights were used when initializing MT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of MT5ForConditionalGeneration were initialized from the model checkpoint at google/mt5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# config & tokenizer\n",
    "if args.model_name_or_path:\n",
    "    config = AutoConfig.from_pretrained(args.model_name_or_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, use_fast=not args.use_slow_tokenizer)\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"Please make sure model_name_or_path is not None\"\n",
    "    )\n",
    "    \n",
    "\n",
    "# mt5-small model\n",
    "if args.model_name_or_path:\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        args.model_name_or_path,\n",
    "        from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
    "        config=config,\n",
    "    )\n",
    "else:\n",
    "    logger.info(\"Training new model from scratch\")\n",
    "    model = AutoModelForSeq2SeqLM.from_config(config)\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "if model.config.decoder_start_token_id is None:\n",
    "    raise ValueError(\"Make sure that `config.decoder_start_token_id` is correctly defined\")\n",
    "\n",
    "prefix = args.source_prefix if args.source_prefix is not None else \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = raw_datasets[\"train\"].column_names\n",
    "dataset_columns = {\"text_column\": \"maintext\", \"summary_column\": \"title\"}\n",
    "label_pad_token_id = -100 if args.ignore_pad_token_for_loss else tokenizer.pad_token_id\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    if args.do_eval:\n",
    "        inputs = examples[dataset_columns[\"text_column\"]]\n",
    "    elif args.do_train:\n",
    "        inputs = examples[dataset_columns[\"text_column\"]]\n",
    "        targets = examples[dataset_columns[\"summary_column\"]]\n",
    "        \n",
    "    inputs = [prefix + inp for inp in inputs]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, max_length=args.max_source_length, padding=\"max_length\", truncation=True)\n",
    "    \n",
    "    if args.do_train:\n",
    "        # Setup the tokenizer for targets\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(targets, max_length=args.max_target_length,\n",
    "                            padding=\"max_length\", truncation=True)\n",
    "\n",
    "        # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore padding in the loss.\n",
    "        if args.ignore_pad_token_for_loss:\n",
    "            labels[\"input_ids\"] = [\n",
    "                [(l if l != tokenizer.pad_token_id else label_pad_token_id) for l in label] for label in labels[\"input_ids\"]\n",
    "            ]\n",
    "\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "def calculate_rouge(preds, refs, avg=True):\n",
    "    preds = [pred.strip() +'\\n' for pred in preds]\n",
    "    refs = [ref.strip() +'\\n' for ref in refs]\n",
    "    return get_rouge(preds, refs, avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:10<00:00,  2.19ba/s]\n",
      "Running tokenizer on dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:02<00:00,  2.44ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 21710\n",
      "})\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 5494\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# First we tokenize all the texts\n",
    "\n",
    "with accelerator.main_process_first():\n",
    "    processed_datasets = raw_datasets.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        num_proc=args.preprocessing_num_workers,\n",
    "        remove_columns=column_names,\n",
    "        load_from_cache_file=not args.overwrite_cache,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    ")\n",
    "\n",
    "train_dataset = processed_datasets[\"train\"]\n",
    "eval_dataset = processed_datasets[\"validation\"]\n",
    "\n",
    "print(train_dataset)\n",
    "print(eval_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset, shuffle=False, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bosh/anaconda3/envs/adl/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Split weights in two groups, one with weight decay and the other not.\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": args.weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n",
    "\n",
    "# Scheduler and math around the number of training steps.\n",
    "num_update_steps_per_epoch = math.ceil(\n",
    "    len(train_dataloader) / args.gradient_accumulation_steps) \n",
    "\n",
    "# args.max_train_steps = epoch * (num of data)/(batch size) / args.gradient_accumulation_steps\n",
    "if args.max_train_steps is None:\n",
    "    args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
    "else:\n",
    "    args.num_train_epochs = math.ceil(\n",
    "        args.max_train_steps / num_update_steps_per_epoch)\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=args.lr_scheduler_type,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=args.num_warmup_steps,\n",
    "    num_training_steps=args.max_train_steps,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare everything with our accelerator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/30/2022 18:22:06 - INFO - __main__ - ***** Running training *****\n",
      "04/30/2022 18:22:06 - INFO - __main__ -   Num examples = 21710\n",
      "04/30/2022 18:22:06 - INFO - __main__ -   Num Epochs = 1\n",
      "04/30/2022 18:22:06 - INFO - __main__ -   Instantaneous batch size per device = 4\n",
      "04/30/2022 18:22:06 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "04/30/2022 18:22:06 - INFO - __main__ -   Gradient Accumulation steps = 4\n",
      "04/30/2022 18:22:06 - INFO - __main__ -   Total optimization steps = 1357\n",
      "1358it [05:33,  4.35it/s]                          Configuration saved in ./mt5_small_model/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge-1: {'r': 0.12268706608332361, 'p': 0.32272470717584084, 'f': 0.16191912815909904}\n",
      "rouge-2: {'r': 0.04514303060386007, 'p': 0.12627125646634657, 'f': 0.059596704048140305}\n",
      "rouge-l: {'r': 0.11579522019651808, 'p': 0.3094296574220564, 'f': 0.15352898939504617}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./mt5_small_model/pytorch_model.bin\n",
      "tokenizer config file saved in ./mt5_small_model/tokenizer_config.json\n",
      "Special tokens file saved in ./mt5_small_model/special_tokens_map.json\n",
      "Copy vocab file to ./mt5_small_model/spiece.model\n"
     ]
    }
   ],
   "source": [
    "# Train!\n",
    "total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n",
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
    "logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n",
    "logger.info(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n",
    "logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
    "logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
    "logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n",
    "\n",
    "progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n",
    "completed_steps = 0\n",
    "starting_epoch = 0\n",
    "best_eval_rouge = 0\n",
    "eval_rouge_history = {\"rouge-1\":[], \"rouge-2\":[], \"rouge-l\":[]}\n",
    "\n",
    "for epoch in range(starting_epoch, args.num_train_epochs):\n",
    "    # Train\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss = loss / args.gradient_accumulation_steps\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        # update parameters (weight and bias) \n",
    "        if step % args.gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "            completed_steps += 1\n",
    "    \n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    # greedy decoding by calling greedy_search() if num_beams=1 and do_sample=False.\n",
    "    # multinomial sampling by calling sample() if num_beams=1 and do_sample=True.\n",
    "    # beam-search decoding by calling beam_search() if num_beams>1 and do_sample=False.\n",
    "    # beam-search multinomial sampling by calling beam_sample() if num_beams>1 and do_sample=True.\n",
    "    # diverse beam-search decoding by calling group_beam_search(), if num_beams>1 and num_beam_groups>1.\n",
    "    # constrained beam-search decoding by calling constrained_beam_search(), if constraints!=None or force_words_ids!=None.\n",
    "    gen_kwargs = {\n",
    "        \"max_length\": args.max_target_length,\n",
    "        \"num_beams\": args.num_beams,\n",
    "    }\n",
    "    preds = []\n",
    "    refs = raw_datasets[\"validation\"][\"title\"]\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = accelerator.unwrap_model(model).generate(\n",
    "                batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                **gen_kwargs,\n",
    "            )\n",
    "            generated_tokens = accelerator.pad_across_processes(\n",
    "                generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n",
    "            )\n",
    "            generated_tokens = accelerator.gather(generated_tokens)\n",
    "            generated_tokens = generated_tokens.cpu().numpy()\n",
    "            \n",
    "            # decode to sentences\n",
    "            decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "            preds += decoded_preds\n",
    "    \n",
    "    rouge_score = calculate_rouge(preds, refs, avg=True)\n",
    "    eval_rouge_history[\"rouge-1\"].append(rouge_score['rouge-1']['f'])\n",
    "    eval_rouge_history[\"rouge-2\"].append(rouge_score['rouge-2']['f'])\n",
    "    eval_rouge_history[\"rouge-l\"].append(rouge_score['rouge-l']['f'])\n",
    "    eval_rouge = rouge_score['rouge-1']['f'] + rouge_score['rouge-2']['f'] + rouge_score['rouge-l']['f']\n",
    "    print(f\"rouge-1: {rouge_score['rouge-1']}\")\n",
    "    print(f\"rouge-2: {rouge_score['rouge-2']}\")\n",
    "    print(f\"rouge-l: {rouge_score['rouge-l']}\")\n",
    "    \n",
    "    # store the best model\n",
    "    if (eval_rouge > best_eval_rouge):\n",
    "        best_eval_rouge = eval_rouge\n",
    "        if args.output_dir is not None:\n",
    "            accelerator.wait_for_everyone()\n",
    "            unwrapped_model = accelerator.unwrap_model(model)\n",
    "            unwrapped_model.save_pretrained(args.output_dir, save_function=accelerator.save)\n",
    "            if accelerator.is_main_process:\n",
    "                tokenizer.save_pretrained(args.output_dir)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save eval rouge history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(args.output_dir, \"eval_rouge.json\"), \"w\") as f:\n",
    "    json.dump(eval_rouge_history, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aaed0f13cab955ee754a66aa5a48de1ce31e05bf25215437ce503315f7004fd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('adl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
